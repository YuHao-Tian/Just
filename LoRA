{
  "coco_detjson_train": {
    "file_name": "train2017_detjson_abs.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  },
  "coco_detjson_val": {
    "file_name": "val2017_detjson_abs.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  }
}




——————————————————————————————————————————————————
# 随机抽两行确认 images 为“单元素列表 + 绝对路径”
python - <<'PY'
import json, os, random
for p in ["/home/vipuser/lf_data/train2017_detjson_abs.jsonl",
          "/home/vipuser/lf_data/val2017_detjson_abs.jsonl"]:
    with open(p) as f:
        lines = [next(f) for _ in range(2)]
    for line in lines:
        o = json.loads(line)
        imgs = o.get("images", [])
        assert isinstance(imgs, list) and len(imgs)==1, "images must be single-item list"
        assert imgs[0].startswith("/home/vipuser/coco/images/") and os.path.exists(imgs[0])
print("OK: images column looks good.")
PY


————————————————————————————————————————————————————————————
source ~/llavaenv/bin/activate

llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --output_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB

___________________________________
ValueError: The number of images does not match the number of <image> tokens in [{'content': '<image>\nYou are an object detection assistant. Return ONLY a valid JSON object with key \'detections\'. Each item: {"label": <one of COCO-80>, "box": [x1,y1,x2,y2], "confidence": [0,1]}. Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. If nothing is found, return {"detections":[]}. Output JSON only.', 'role': 'user'}, {'content': '{"detections": [{"label": "motorcycle", "box": [0.561203125, 0.40602777777777777, 0.73690625, 0.9992777777777778], "confidence": 1.0}, {"label": "person", "box": [0.5310625, 0.06155555555555556, 0.7715, 0.8969166666666668], "confidence": 1.0}, {"label": "person", "box": [0.7369375, 0.4800555555555555, 0.7930625, 0.6136666666666666], "confidence": 1.0}, {"label": "bicycle", "box": [0.759390625, 0.5091944444444445, 0.80725, 0.6063611111111111], "confidence": 1.0}]}', 'role': 'assistant'}].

________________________
   {"id": "coco_train2017_391895", "images": ["/home/vipuser/coco/images/train2017/000000391895.jpg"], "conversations": [{"from": "user", "value": "<image>\nYou are an object detection assistant. Return ONLY a valid JSON object with key 'detections'. Each item: {\"label\": <one of COCO-80>, \"box\": [x1,y1,x2,y2], \"confidence\": [0,1]}. Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. If nothing is found, return {\"detections\":[]}. Output JSON only."}, {"from": "assistant", "value": "{\"detections\": [{\"label\": \"motorcycle\", \"box\": [0.561203125, 0.40602777777777777, 0.73690625, 0.9992777777777778], \"confidence\": 1.0}, {\"label\": \"person\", \"box\": [0.5310625, 0.06155555555555556, 0.7715, 0.8969166666666668], \"confidence\": 1.0}, {\"label\": \"person\", \"box\": [0.7369375, 0.4800555555555555, 0.7930625, 0.6136666666666666], \"confidence\": 1.0}, {\"label\": \"bicycle\", \"box\": [0.759390625, 0.5091944444444445, 0.80725, 0.6063611111111111], \"confidence\": 1.0}]}"}]}
_____________________
[INFO|modeling_utils.py:2241] 2025-08-18 23:44:01,502 >> Instantiating LlamaModel model under default dtype torch.float16.
Loading checkpoint shards: 100%|██████████████████| 3/3 [00:38<00:00, 12.74s/it]
[INFO|modeling_utils.py:5131] 2025-08-18 23:44:39,769 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-08-18 23:44:39,769 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at /home/vipuser/llava-1.5-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-08-18 23:44:39,774 >> loading configuration file /home/vipuser/llava-1.5-7b-hf/generation_config.json
[INFO|configuration_utils.py:1135] 2025-08-18 23:44:39,774 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 32001
}

[INFO|2025-08-18 23:44:39] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-18 23:44:39] llamafactory.model.loader:143 >> all params: 7,063,427,072
——————————————————————————————————
source ~/llavaenv/bin/activate

python -u -m llamafactory.cli.train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --num_train_epochs 3 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  --preprocessing_num_workers 4 \
  --dataloader_num_workers 2 \
  | tee /home/vipuser/saves/quick_sanity.log

____________________
/home/vipuser/llavaenv/bin/python: Error while finding module specification for 'llamafactory.cli.train' (ModuleNotFoundError: __path__ attribute not found on 'llamafactory.cli' while trying to find 'llamafactory.cli.train')

——————————————
source ~/llavaenv/bin/activate

# 确保用的就是这个环境里的 CLI
which llamafactory-cli

# 用 CLI 跑一个 5 步 sanity check
/home/vipuser/llavaenv/bin/llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --num_train_epochs 3 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  --preprocessing_num_workers 4 \
  --dataloader_num_workers 2 \
  -u | tee /home/vipuser/saves/quick_sanity.log

________________
Got unknown args, potentially deprecated arguments: ['-u']
Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 55, in _training_function
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 208, in get_train_args
    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 186, in _parse_train_args
    return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 87, in _parse_args
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {unknown_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['-u']

————————————————
source ~/llavaenv/bin/activate

/home/vipuser/llavaenv/bin/llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  2>&1 | tee /home/vipuser/saves/quick_sanity.log
____________________

[INFO|modeling_utils.py:2241] 2025-08-19 00:02:32,433 >> Instantiating CLIPVisionModel model under default dtype torch.float16.
[INFO|modeling_utils.py:2241] 2025-08-19 00:02:32,471 >> Instantiating LlamaModel model under default dtype torch.float16.
Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 10.98s/it]
[INFO|modeling_utils.py:5131] 2025-08-19 00:03:05,454 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-08-19 00:03:05,454 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at /home/vipuser/llava-1.5-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-08-19 00:03:05,459 >> loading configuration file /home/vipuser/llava-1.5-7b-hf/generation_config.json
[INFO|configuration_utils.py:1135] 2025-08-19 00:03:05,460 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 32001
}

[INFO|2025-08-19 00:03:05] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-08-19 00:03:05] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-19 00:03:05] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-08-19 00:03:05] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-08-19 00:03:05] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,k_proj,down_proj,q_proj,o_proj,v_proj,up_proj
[INFO|2025-08-19 00:03:05] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['vision_tower'].
[INFO|2025-08-19 00:03:05] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: multi_modal_projector.
[INFO|2025-08-19 00:03:06] llamafactory.model.loader:143 >> trainable params: 19,988,480 || all params: 7,083,415,552 || trainable%: 0.2822
[INFO|trainer.py:706] 2025-08-19 00:03:06,664 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2409] 2025-08-19 00:03:07,029 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-08-19 00:03:07,029 >>   Num examples = 118,287
[INFO|trainer.py:2411] 2025-08-19 00:03:07,029 >>   Num Epochs = 1
[INFO|trainer.py:2412] 2025-08-19 00:03:07,029 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2415] 2025-08-19 00:03:07,029 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2416] 2025-08-19 00:03:07,029 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2417] 2025-08-19 00:03:07,029 >>   Total optimization steps = 5
[INFO|trainer.py:2418] 2025-08-19 00:03:07,033 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/5 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/sft/workflow.py", line 96, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 3745, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/sft/trainer.py", line 103, in compute_loss
    return super().compute_loss(model, inputs, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 3810, in compute_loss
    outputs = model(**inputs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/peft/peft_model.py", line 1757, in forward
    return self.base_model(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py", line 438, in forward
    outputs = self.model(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py", line 299, in forward
    raise ValueError(
ValueError: Image features and image tokens do not match: tokens: 1115, features 1152
  0%|          | 0/5 [00:01<?, ?it/s]

————————————————————
rm -rf ~/.cache/huggingface/datasets/llamafactory_* ~/.cache/llamafactory/*

source ~/llavaenv/bin/activate

/home/vipuser/llavaenv/bin/llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 4096 \
  --learning_rate 5e-05 \
  --num_train_epochs 1 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  --overwrite_cache true \
  --remove_unused_columns false \
  --packing false \
  2>&1 | tee /home/vipuser/saves/quick_sanity.log

________________
[INFO|trainer.py:706] 2025-08-19 00:28:50,993 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:2409] 2025-08-19 00:28:51,243 >> ***** Running training *****
[INFO|trainer.py:2410] 2025-08-19 00:28:51,243 >>   Num examples = 118,287
[INFO|trainer.py:2411] 2025-08-19 00:28:51,243 >>   Num Epochs = 1
[INFO|trainer.py:2412] 2025-08-19 00:28:51,243 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2415] 2025-08-19 00:28:51,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2416] 2025-08-19 00:28:51,243 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2417] 2025-08-19 00:28:51,243 >>   Total optimization steps = 5
[INFO|trainer.py:2418] 2025-08-19 00:28:51,247 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/5 [00:00<?, ?it/s]Could not load library libcuda.so. Error: libcuda.so: cannot open shared object file: No such file or directory
Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/sft/workflow.py", line 96, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 2240, in train
    return inner_training_loop(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 2555, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 3745, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/sft/trainer.py", line 103, in compute_loss
    return super().compute_loss(model, inputs, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/trainer.py", line 3810, in compute_loss
    outputs = model(**inputs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/peft/peft_model.py", line 1757, in forward
    return self.base_model(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py", line 438, in forward
    outputs = self.model(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llava/modeling_llava.py", line 305, in forward
    outputs = self.language_model(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/utils/generic.py", line 969, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 453, in forward
    layer_outputs = decoder_layer(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/modeling_layers.py", line 47, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
    return gradient_checkpointing_func(func, *args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 308, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 265, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/transformers/integrations/sdpa_attention.py", line 54, in sdpa_attention_forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
RuntimeError: cuDNN Frontend error: [cudnn_frontend] Error: No execution plans support the graph.
  0%|          | 0/5 [00:01<?, ?it/s]

