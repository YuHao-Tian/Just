cp /home/vipuser/lf_data/train2017_detjson.jsonl /home/vipuser/lf_data/train2017_detjson.bak.jsonl
cp /home/vipuser/lf_data/val2017_detjson.jsonl   /home/vipuser/lf_data/val2017_detjson.bak.jsonl

# 训练集
python - <<'PY'
import json, sys, os
inp = "/home/vipuser/lf_data/train2017_detjson.jsonl"
out = "/home/vipuser/lf_data/train2017_detjson_abs.jsonl"
root = "/home/vipuser/coco/images"
n = 0
with open(inp, "r") as fin, open(out, "w") as fout:
    for line in fin:
        obj = json.loads(line)
        imgs = obj.get("images", [])
        new_imgs = []
        for p in imgs:
            if not p.startswith("/"):
                p = os.path.join(root, p)  # -> /home/coco/images/train2017/xxx.jpg
            new_imgs.append(p)
        obj["images"] = new_imgs
        fout.write(json.dumps(obj, ensure_ascii=False) + "\n")
        n += 1
print(f"[done] {n} lines -> {out}")
PY

# 验证集
python - <<'PY'
import json, sys, os
inp = "/home/vipuser/lf_data/val2017_detjson.jsonl"
out = "/home/vipuser/lf_data/val2017_detjson_abs.jsonl"
root = "/home/vipuser/coco/images"
n = 0
with open(inp, "r") as fin, open(out, "w") as fout:
    for line in fin:
        obj = json.loads(line)
        imgs = obj.get("images", [])
        new_imgs = []
        for p in imgs:
            if not p.startswith("/"):
                p = os.path.join(root, p)
            new_imgs.append(p)
        obj["images"] = new_imgs
        fout.write(json.dumps(obj, ensure_ascii=False) + "\n")
        n += 1
print(f"[done] {n} lines -> {out}")
PY



head -n 1 /home/vipuser/lf_data/train2017_detjson_abs.jsonl
head -n 1 /home/vipuser/lf_data/val2017_detjson_abs.jsonl



{
  "coco_detjson_train": {
    "file_name": "train2017_detjson_abs.jsonl",
    "format": "sharegpt",
    "columns": { "messages": "conversations" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true,
    "image_field": "images"
  },
  "coco_detjson_val": {
    "file_name": "val2017_detjson_abs.jsonl",
    "format": "sharegpt",
    "columns": { "messages": "conversations" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true,
    "image_field": "images"
  }
}


source ~/llavaenv/bin/activate

llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --output_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB



training example:
Traceback (most recent call last):
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/data/loader.py", line 267, in _get_preprocessed_dataset
    dataset_processor.print_data_example(next(iter(dataset)))
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/sft/workflow.py", line 51, in run_sft
    dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/data/loader.py", line 315, in get_dataset
    dataset = _get_preprocessed_dataset(
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/data/loader.py", line 272, in _get_preprocessed_dataset
    raise RuntimeError("Cannot find valid samples, check `data/README.md` for the data format.")
RuntimeError: Cannot find valid samples, check `data/README.md` for the data format.
