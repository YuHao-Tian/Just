{
  "coco_detjson_train": {
    "file_name": "train2017_detjson_abs.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  },
  "coco_detjson_val": {
    "file_name": "val2017_detjson_abs.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  }
}




——————————————————————————————————————————————————
# 随机抽两行确认 images 为“单元素列表 + 绝对路径”
python - <<'PY'
import json, os, random
for p in ["/home/vipuser/lf_data/train2017_detjson_abs.jsonl",
          "/home/vipuser/lf_data/val2017_detjson_abs.jsonl"]:
    with open(p) as f:
        lines = [next(f) for _ in range(2)]
    for line in lines:
        o = json.loads(line)
        imgs = o.get("images", [])
        assert isinstance(imgs, list) and len(imgs)==1, "images must be single-item list"
        assert imgs[0].startswith("/home/vipuser/coco/images/") and os.path.exists(imgs[0])
print("OK: images column looks good.")
PY


————————————————————————————————————————————————————————————
source ~/llavaenv/bin/activate

llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --output_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB

___________________________________
ValueError: The number of images does not match the number of <image> tokens in [{'content': '<image>\nYou are an object detection assistant. Return ONLY a valid JSON object with key \'detections\'. Each item: {"label": <one of COCO-80>, "box": [x1,y1,x2,y2], "confidence": [0,1]}. Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. If nothing is found, return {"detections":[]}. Output JSON only.', 'role': 'user'}, {'content': '{"detections": [{"label": "motorcycle", "box": [0.561203125, 0.40602777777777777, 0.73690625, 0.9992777777777778], "confidence": 1.0}, {"label": "person", "box": [0.5310625, 0.06155555555555556, 0.7715, 0.8969166666666668], "confidence": 1.0}, {"label": "person", "box": [0.7369375, 0.4800555555555555, 0.7930625, 0.6136666666666666], "confidence": 1.0}, {"label": "bicycle", "box": [0.759390625, 0.5091944444444445, 0.80725, 0.6063611111111111], "confidence": 1.0}]}', 'role': 'assistant'}].

________________________
   {"id": "coco_train2017_391895", "images": ["/home/vipuser/coco/images/train2017/000000391895.jpg"], "conversations": [{"from": "user", "value": "<image>\nYou are an object detection assistant. Return ONLY a valid JSON object with key 'detections'. Each item: {\"label\": <one of COCO-80>, \"box\": [x1,y1,x2,y2], \"confidence\": [0,1]}. Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. If nothing is found, return {\"detections\":[]}. Output JSON only."}, {"from": "assistant", "value": "{\"detections\": [{\"label\": \"motorcycle\", \"box\": [0.561203125, 0.40602777777777777, 0.73690625, 0.9992777777777778], \"confidence\": 1.0}, {\"label\": \"person\", \"box\": [0.5310625, 0.06155555555555556, 0.7715, 0.8969166666666668], \"confidence\": 1.0}, {\"label\": \"person\", \"box\": [0.7369375, 0.4800555555555555, 0.7930625, 0.6136666666666666], \"confidence\": 1.0}, {\"label\": \"bicycle\", \"box\": [0.759390625, 0.5091944444444445, 0.80725, 0.6063611111111111], \"confidence\": 1.0}]}"}]}
_____________________
[INFO|modeling_utils.py:2241] 2025-08-18 23:44:01,502 >> Instantiating LlamaModel model under default dtype torch.float16.
Loading checkpoint shards: 100%|██████████████████| 3/3 [00:38<00:00, 12.74s/it]
[INFO|modeling_utils.py:5131] 2025-08-18 23:44:39,769 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.

[INFO|modeling_utils.py:5139] 2025-08-18 23:44:39,769 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at /home/vipuser/llava-1.5-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1088] 2025-08-18 23:44:39,774 >> loading configuration file /home/vipuser/llava-1.5-7b-hf/generation_config.json
[INFO|configuration_utils.py:1135] 2025-08-18 23:44:39,774 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 32001
}

[INFO|2025-08-18 23:44:39] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-18 23:44:39] llamafactory.model.loader:143 >> all params: 7,063,427,072
——————————————————————————————————
source ~/llavaenv/bin/activate

python -u -m llamafactory.cli.train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --num_train_epochs 3 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  --preprocessing_num_workers 4 \
  --dataloader_num_workers 2 \
  | tee /home/vipuser/saves/quick_sanity.log

____________________
/home/vipuser/llavaenv/bin/python: Error while finding module specification for 'llamafactory.cli.train' (ModuleNotFoundError: __path__ attribute not found on 'llamafactory.cli' while trying to find 'llamafactory.cli.train')

——————————————
source ~/llavaenv/bin/activate

# 确保用的就是这个环境里的 CLI
which llamafactory-cli

# 用 CLI 跑一个 5 步 sanity check
/home/vipuser/llavaenv/bin/llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --num_train_epochs 3 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  --preprocessing_num_workers 4 \
  --dataloader_num_workers 2 \
  -u | tee /home/vipuser/saves/quick_sanity.log

________________
Got unknown args, potentially deprecated arguments: ['-u']
Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 55, in _training_function
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 208, in get_train_args
    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 186, in _parse_train_args
    return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 87, in _parse_args
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {unknown_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['-u']

————————————————
source ~/llavaenv/bin/activate

/home/vipuser/llavaenv/bin/llamafactory-cli train \
  --stage sft \
  --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data \
  --dataset coco_detjson_train \
  --eval_dataset coco_detjson_val \
  --template llava \
  --cutoff_len 1024 \
  --learning_rate 5e-05 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --lora_rank 8 \
  --lora_alpha 16 \
  --lora_dropout 0.1 \
  --lora_target all \
  --do_train true \
  --do_eval false \
  --max_steps 5 \
  --logging_steps 1 \
  --report_to none \
  --overwrite_output_dir true \
  2>&1 | tee /home/vipuser/saves/quick_sanity.log
