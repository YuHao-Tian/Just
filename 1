# /home/vipuser/image_caption.py
import sys, torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration

MODEL_DIR = "/home/vipuser/llava-1.5-7b-hf"

# 命令行参数：image_path [可选: 自定义提示词]
image_path = sys.argv[1] if len(sys.argv) > 1 else "test.jpg"
prompt_text = sys.argv[2] if len(sys.argv) > 2 else "用中文一句话描述这张图。"

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device == "cuda" else torch.float32  # A100 推荐 bfloat16

# 加载模型与处理器
model = LlavaForConditionalGeneration.from_pretrained(
    MODEL_DIR, torch_dtype=dtype, low_cpu_mem_usage=True
).eval().to(device)
processor = AutoProcessor.from_pretrained(MODEL_DIR)

# 读图
img = Image.open(image_path).convert("RGB")

# 构造“图+问句”的聊天模板（关键：包含一条 image）
messages = [{
    "role": "user",
    "content": [
        {"type": "image"},
        {"type": "text", "text": prompt_text}
]}]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)

# 编码并生成
inputs = processor(images=img, text=prompt, return_tensors="pt")
inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}

with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)

# 解码；部分模板前面会带 "ASSISTANT:"，这里顺便剥掉
text = processor.decode(outputs[0], skip_special_tokens=True)
print(text.split("ASSISTANT:")[-1].strip())

PROMPT:
python /home/vipuser/image_caption.py /path/to/your.jpg "用中文一句话描述这张图。"


# 1) 安装 Miniforge（只装到你的 home，不动系统）
cd ~
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh
bash Miniforge3-Linux-x86_64.sh -b -p $HOME/miniforge3

# 2) 让当前 shell 接管新的 conda
eval "$($HOME/miniforge3/bin/conda shell.bash hook)"
conda -V                  # 应该看到 23/24.x 而不是 4.0.5
which conda               # 应该指向 ~/miniforge3/bin/conda

# 3) 创建你要的环境（随便 3.10/3.11 都有）
conda create -n py310 python=3.10 -y
conda activate py310
python -V                 # Python 3.10.x



--------------------------------------

import os, json, argparse, time, sys
import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def try_parse_json(text: str):
    """Extract outermost JSON object if possible; else return empty schema."""
    try:
        s, e = text.find("{"), text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
    except Exception:
        pass
    return {"detections": []}

def build_canon_label(name2id):
    """Return a canonicalizer mapping raw labels to official COCO-80 names (lower-case)."""
    name_set = set(name2id.keys())
    no_space_map = {n.replace(" ", ""): n for n in name_set}
    syn = {
        "people":"person","man":"person","woman":"person","men":"person","women":"person",
        "boy":"person","girl":"person","kid":"person","child":"person","baby":"person",
        "motorbike":"motorcycle","aeroplane":"airplane","aircraft":"airplane",
        "trafficlight":"traffic light","traffic-light":"traffic light",
        "tvmonitor":"tv","tv monitor":"tv","television":"tv",
        "cellphone":"cell phone","mobile phone":"cell phone","smartphone":"cell phone","iphone":"cell phone",
        "sofa":"couch","pottedplant":"potted plant","potted plant":"potted plant",
        "diningtable":"dining table","hand bag":"handbag","hand-bag":"handbag",
        "wineglass":"wine glass","wine-glass":"wine glass",
        "tennis-racket":"tennis racket","baseball-bat":"baseball bat","baseball-glove":"baseball glove"
    }
    def canon(raw: str):
        s = (raw or "").lower().strip()
        s = s.replace("_"," ").replace("-"," ")
        while "  " in s: s = s.replace("  ", " ")
        if s in name_set: return s
        if s in syn: return syn[s]
        key = s.replace(" ","")
        if key in no_space_map: return no_space_map[key]
        return None
    return canon

def gen_and_decode_reply(model, proc, img, prompt, device, max_new_tokens=192):
    """Generate once and decode ONLY the assistant's newly generated text."""
    inputs = proc(images=img, text=prompt, return_tensors="pt")
    inputs = {k:(v.to(device) if hasattr(v,"to") else v) for k,v in inputs.items()}
    with torch.inference_mode():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            return_dict_in_generate=True
        )
    seq = out.sequences[0]
    # 只解码新生成的部分
    gen_ids = seq[inputs["input_ids"].shape[-1]:]
    return proc.decode(gen_ids, skip_special_tokens=True).strip()

def labels_invalid(dets, allowed_lower: set):
    """Return True if any label is clearly invalid (placeholder or not in allowed set)."""
    for d in dets:
        lab = str(d.get("label","")).lower()
        if ("<" in lab) or (lab not in allowed_lower):
            return True
    return False

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model-dir", default="/home/vipuser/llava-1.5-7b-hf")
    ap.add_argument("--val-ann",  default="/home/vipuser/coco/annotations/instances_val2017.json")
    ap.add_argument("--val-img",  default="/home/vipuser/coco/images/val2017")
    ap.add_argument("--subset",   type=int, default=10, help="evaluate first N images (0 = all 5000)")
    ap.add_argument("--tokens",   type=int, default=192)
    ap.add_argument("--out",      default="/home/vipuser/coco/llava_dt_base.json")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype  = torch.bfloat16 if device == "cuda" else torch.float32

    # 1) Load base LLaVA-1.5-7B (no LoRA here)
    model = LlavaForConditionalGeneration.from_pretrained(args.model_dir, torch_dtype=dtype).to(device).eval()
    proc  = AutoProcessor.from_pretrained(args.model_dir)

    # 2) COCO classes & images
    coco = COCO(args.val_ann)
    cats = coco.loadCats(coco.getCatIds())
    class_names = [c["name"] for c in cats]              # official 80 names
    name2id = {c["name"].lower(): c["id"] for c in cats}
    canon_label = build_canon_label(name2id)
    allowed = set([n.lower() for n in class_names])

    imgs = coco.dataset["images"]
    ids  = [im["id"] for im in imgs]
    info = {im["id"]: (im["file_name"], im["width"], im["height"]) for im in imgs}
    if args.subset and args.subset > 0:
        ids = ids[:args.subset]
        print(f"[info] subset = {len(ids)} images")

    # 3) English instruction (NO placeholders, NO inline JSON example)
    instr = (
        "You are an object detection assistant. "
        "Return ONLY a valid JSON object with key 'detections'. "
        "Each item is: {\"label\": <one of the COCO-80 list>, \"box\": [x1,y1,x2,y2], \"confidence\": number in [0,1]}. "
        "Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. "
        "If nothing is found, return {\"detections\":[]}. "
        "Use ONLY these labels (singular, English): " + ", ".join(class_names) + ". "
        "At most 15 objects. Output JSON only."
    )

    dt = []
    ok_json = 0
    t0 = time.time()

    for k, img_id in enumerate(ids, 1):
        fn, W, H = info[img_id]
        img_path = os.path.join(args.val_img, fn)
        img = Image.open(img_path).convert("RGB")

        # Build chat messages: one image + one text instruction
        messages = [{"role":"user","content":[
            {"type":"image"},
            {"type":"text","text": instr}
        ]}]
        prompt = proc.apply_chat_template(messages, add_generation_prompt=True)

        # First try
        txt  = gen_and_decode_reply(model, proc, img, prompt, device, max_new_tokens=args.tokens)
        if k <= 5:
            print(f"==== RAW REPLY (image {k}: {fn}) ====")
            print(txt)

        data = try_parse_json(txt)

        # Retry if JSON invalid
        if not isinstance(data.get("detections", None), list):
            retry = [{"role":"user","content":[
                {"type":"image"},
                {"type":"text","text":"Only output valid JSON with the key 'detections'. No extra words."}
            ]}]
            rprompt = proc.apply_chat_template(retry, add_generation_prompt=True)
            txt2 = gen_and_decode_reply(model, proc, img, rprompt, device, max_new_tokens=128)
            if k <= 5:
                print(f"==== RETRY JSON (image {k}) ====")
                print(txt2)
            data = try_parse_json(txt2)

        dets = data.get("detections", [])
        # Retry if labels look invalid (e.g., placeholders or not in COCO-80)
        if isinstance(dets, list) and labels_invalid(dets, allowed):
            retry2 = [{"role":"user","content":[
                {"type":"image"},
                {"type":"text","text":
                 "Rewrite as JSON using ONLY these labels: " + ", ".join(class_names) +
                 ". Do NOT use placeholders. Output JSON only."}
            ]}]
            rprompt2 = proc.apply_chat_template(retry2, add_generation_prompt=True)
            txt_fix = gen_and_decode_reply(model, proc, img, rprompt2, device, max_new_tokens=128)
            if k <= 5:
                print(f"==== RETRY LABELS (image {k}) ====")
                print(txt_fix)
            data = try_parse_json(txt_fix)
            dets = data.get("detections", [])

        # Count JSON-parsable replies
        if isinstance(dets, list):
            ok_json += 1

        # Convert normalized boxes to COCO bbox and append detections
        for d in dets[:15]:
            if not isinstance(d, dict) or "box" not in d:
                continue
            lab = canon_label(d.get("label", ""))
            if lab is None:
                continue
            try:
                x1, y1, x2, y2 = d["box"]
            except Exception:
                continue
            x, y, w, h = x1 * W, y1 * H, (x2 - x1) * W, (y2 - y1) * H
            dt.append({
                "image_id": int(img_id),
                "category_id": name2id[lab],
                "bbox": [float(x), float(y), float(w), float(h)],
                "score": float(d.get("confidence", 0.9))
            })

        print(f"[{k}/{len(ids)}] dt={len(dt)}")

    # Save detections & report JSON success rate
    with open(args.out, "w") as f:
        json.dump(dt, f)
    print(f"[saved] detections -> {args.out}")
    jsucc = ok_json / len(ids) if len(ids) else 0.0
    print(f"[info] JSON success rate: {ok_json}/{len(ids)} = {jsucc:.2%}")
    print(f"[time] processed {len(ids)} images in {time.time()-t0:.1f}s")

    # COCO evaluation
    if len(dt) == 0:
        print("[warn] No detections recorded. mAP will be 0.0. Check RAW/RETRY outputs above.")
        sys.exit(0)

    res = coco.loadRes(args.out)
    e = COCOeval(coco, res, "bbox")
    e.evaluate(); e.accumulate(); e.summarize()
    mAP   = float(e.stats[0])   # AP@[.50:.95]
    AP50  = float(e.stats[1])   # AP@0.50
    AR100 = float(e.stats[8])   # AR@100
    print(f"[metrics] mAP@[.50:.95]={mAP:.4f} | AP50={AP50:.4f} | AR@100={AR100:.4f}")

if __name__ == "__main__":
    main()
--------------------------------
source ~/llavaenv/bin/activate
pip install -U pycocotools

python /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/llava-1.5-7b-hf \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 10 \
  --out /home/vipuser/coco/llava_dt_base.json


----------------------------------------
import os, json, argparse, time, sys, re
import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

# ---------- utilities ----------
def try_parse_json(text: str):
    try:
        s, e = text.find("{"), text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
    except Exception:
        pass
    return {"detections": []}

def to_float(x, default=0.0):
    """Coerce x -> float in [0,1] if possible. Handle list/str/%/>1."""
    try:
        if isinstance(x, list) and len(x) > 0:
            x = x[0]
        if isinstance(x, str):
            x = x.strip().replace("%", "")
        v = float(x)
        if v > 1.0:
            # assume 0~100 percentage
            v = v / 100.0 if v <= 100.0 else 1.0
        if v < 0.0:
            v = 0.0
        if v > 1.0:
            v = 1.0
        return v
    except Exception:
        return default

def to_box(b):
    """Coerce various box formats to [x1,y1,x2,y2] in [0,1]; return None if fail."""
    def flat4(lst):
        out = []
        for el in lst:
            if isinstance(el, (list, tuple)):
                out.extend(flat4(el))
            else:
                out.append(el)
        return out
    if isinstance(b, dict):
        xs = [b.get("x1",0), b.get("y1",0), b.get("x2",0), b.get("y2",0)]
        x1,y1,x2,y2 = [to_float(v,0.0) for v in xs]
    elif isinstance(b, (list, tuple)):
        lst = flat4(list(b))
        if len(lst) < 4: return None
        x1,y1,x2,y2 = [to_float(lst[i],0.0) for i in range(4)]
    elif isinstance(b, str):
        nums = re.findall(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?", b)
        if len(nums) < 4: return None
        x1,y1,x2,y2 = [to_float(nums[i],0.0) for i in range(4)]
    else:
        return None
    # clamp to [0,1] and fix order
    x1 = min(max(x1,0.0),1.0); y1 = min(max(y1,0.0),1.0)
    x2 = min(max(x2,0.0),1.0); y2 = min(max(y2,0.0),1.0)
    if x2 < x1: x1, x2 = x2, x1
    if y2 < y1: y1, y2 = y2, y1
    if (x2 - x1) <= 1e-6 or (y2 - y1) <= 1e-6:
        return None
    return [x1,y1,x2,y2]

def build_canon_label(name2id):
    name_set = set(name2id.keys())
    no_space_map = {n.replace(" ", ""): n for n in name_set}
    syn = {
        "people":"person","man":"person","woman":"person","men":"person","women":"person",
        "boy":"person","girl":"person","kid":"person","child":"person","baby":"person",
        "motorbike":"motorcycle","aeroplane":"airplane","aircraft":"airplane",
        "trafficlight":"traffic light","traffic-light":"traffic light",
        "tvmonitor":"tv","tv monitor":"tv","television":"tv",
        "cellphone":"cell phone","mobile phone":"cell phone","smartphone":"cell phone","iphone":"cell phone",
        "sofa":"couch","pottedplant":"potted plant","potted plant":"potted plant",
        "diningtable":"dining table","hand bag":"handbag","hand-bag":"handbag",
        "wineglass":"wine glass","wine-glass":"wine glass",
        "tennis-racket":"tennis racket","baseball-bat":"baseball bat","baseball-glove":"baseball glove"
    }
    def canon(raw: str):
        s = (raw or "").lower().strip().replace("_"," ").replace("-"," ")
        while "  " in s: s = s.replace("  "," ")
        if s in name_set: return s
        if s in syn: return syn[s]
        key = s.replace(" ","")
        if key in no_space_map: return no_space_map[key]
        return None
    return canon

def gen_and_decode_reply(model, proc, img, prompt, device, max_new_tokens=160):
    inputs = proc(images=img, text=prompt, return_tensors="pt")
    inputs = {k:(v.to(device) if hasattr(v,"to") else v) for k,v in inputs.items()}
    with torch.inference_mode():
        out = model.generate(
            **inputs, max_new_tokens=max_new_tokens, do_sample=False,
            return_dict_in_generate=True
        )
    seq = out.sequences[0]
    gen_ids = seq[inputs["input_ids"].shape[-1]:]  # decode only new tokens
    return proc.decode(gen_ids, skip_special_tokens=True).strip()

def labels_invalid(dets, allowed_lower: set):
    for d in dets:
        lab = str(d.get("label","")).lower()
        if ("<" in lab) or (lab not in allowed_lower):
            return True
    return False

# ---------- main ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model-dir", default="/home/vipuser/llava-1.5-7b-hf")
    ap.add_argument("--val-ann",  default="/home/vipuser/coco/annotations/instances_val2017.json")
    ap.add_argument("--val-img",  default="/home/vipuser/coco/images/val2017")
    ap.add_argument("--subset",   type=int, default=500, help="first N images (0=all 5000)")
    ap.add_argument("--tokens",   type=int, default=160)
    ap.add_argument("--out",      default="/home/vipuser/coco/llava_dt_base.json")
    ap.add_argument("--metrics",  default="/home/vipuser/coco/llava_metrics_base.json")
    ap.add_argument("--raw_debug_first", type=int, default=0)
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype  = torch.bfloat16 if device == "cuda" else torch.float32

    model = LlavaForConditionalGeneration.from_pretrained(args.model_dir, torch_dtype=dtype).to(device).eval()
    proc  = AutoProcessor.from_pretrained(args.model_dir)

    coco = COCO(args.val_ann)
    cats = coco.loadCats(coco.getCatIds())
    class_names = [c["name"] for c in cats]
    name2id = {c["name"].lower(): c["id"] for c in cats}
    canon_label = build_canon_label(name2id)
    allowed = set([n.lower() for n in class_names])

    imgs = coco.dataset["images"]
    ids  = [im["id"] for im in imgs]
    info = {im["id"]:(im["file_name"], im["width"], im["height"]) for im in imgs}
    if args.subset and args.subset > 0:
        ids = ids[:args.subset]
    print(f"[info] subset = {len(ids)} images")

    instr = (
        "You are an object detection assistant. "
        "Return ONLY a valid JSON object with key 'detections'. "
        "Each item: {\"label\": <one of COCO-80>, \"box\": [x1,y1,x2,y2], \"confidence\": [0,1]}. "
        "Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. "
        "If nothing is found, return {\"detections\":[]}. "
        "Use ONLY these labels (singular, English): " + ", ".join(class_names) + ". "
        "At most 15 objects. Output JSON only."
    )

    dt = []
    ok_json = 0
    t0 = time.time()

    for k, img_id in enumerate(ids, 1):
        fn, W, H = info[img_id]
        img = Image.open(os.path.join(args.val_img, fn)).convert("RGB")

        messages = [{"role":"user","content":[{"type":"image"},{"type":"text","text":instr}]}]
        prompt = proc.apply_chat_template(messages, add_generation_prompt=True)

        # First pass
        txt = gen_and_decode_reply(model, proc, img, prompt, device, max_new_tokens=args.tokens)
        if k <= args.raw_debug_first:
            print(f"==== RAW (image {k}: {fn}) ====\n{txt}")
        data = try_parse_json(txt)

        # Retry JSON
        if not isinstance(data.get("detections", None), list):
            retry = [{"role":"user","content":[
                {"type":"image"},
                {"type":"text","text":"Only output valid JSON with the key 'detections'. No extra words."}
            ]}]
            rprompt = proc.apply_chat_template(retry, add_generation_prompt=True)
            txt2 = gen_and_decode_reply(model, proc, img, rprompt, device, max_new_tokens=128)
            if k <= args.raw_debug_first:
                print(f"==== RETRY-JSON (image {k}) ====\n{txt2}")
            data = try_parse_json(txt2)

        dets = data.get("detections", [])
        # Retry labels
        if isinstance(dets, list) and labels_invalid(dets, allowed):
            retry2 = [{"role":"user","content":[
                {"type":"image"},
                {"type":"text","text":
                 "Rewrite as JSON using ONLY these labels: " + ", ".join(class_names) +
                 ". Do NOT use placeholders. Output JSON only."}
            ]}]
            rprompt2 = proc.apply_chat_template(retry2, add_generation_prompt=True)
            txt_fix = gen_and_decode_reply(model, proc, img, rprompt2, device, max_new_tokens=128)
            if k <= args.raw_debug_first:
                print(f"==== RETRY-LABELS (image {k}) ====\n{txt_fix}")
            data = try_parse_json(txt_fix)
            dets = data.get("detections", [])

        if isinstance(dets, list):
            ok_json += 1

        # Convert and append
        for d in dets[:15]:
            if not isinstance(d, dict) or "box" not in d:
                continue
            lab = canon_label(d.get("label", ""))
            if lab is None:
                continue
            box = to_box(d.get("box"))
            if box is None:
                continue
            x1,y1,x2,y2 = box
            x, y, w, h = x1*W, y1*H, (x2-x1)*W, (y2-y1)*H
            if w <= 1e-6 or h <= 1e-6:
                continue
            score = to_float(d.get("confidence", 0.9), default=0.9)
            dt.append({
                "image_id": int(img_id),
                "category_id": name2id[lab],
                "bbox": [float(x), float(y), float(w), float(h)],
                "score": float(score)
            })

        if (k % 50 == 0) or (k == len(ids)):
            print(f"[{k}/{len(ids)}] dt={len(dt)}")

    with open(args.out, "w") as f:
        json.dump(dt, f)
    jsucc = ok_json / len(ids) if len(ids) else 0.0
    print(f"[saved] {args.out}")
    print(f"[info] JSON success rate: {ok_json}/{len(ids)} = {jsucc:.2%}")
    print(f"[time] processed {len(ids)} images in {time.time()-t0:.1f}s")

    metrics = {"mAP": 0.0, "AP50": 0.0, "AP75": 0.0, "AR@1": 0.0, "AR@10": 0.0, "AR@100": 0.0,
               "json_success_rate": jsucc, "images": len(ids), "detections": len(dt)}
    if len(dt) == 0:
        print("[warn] No detections recorded. mAP will be 0.0. Proceed to LoRA finetuning or adjust prompts.")
    else:
        res = coco.loadRes(args.out)
        e = COCOeval(coco, res, "bbox")
        e.evaluate(); e.accumulate(); e.summarize()
        metrics.update({
            "mAP":   float(e.stats[0]),
            "AP50":  float(e.stats[1]),
            "AP75":  float(e.stats[2]),
            "AR@1":  float(e.stats[6]),
            "AR@10": float(e.stats[7]),
            "AR@100":float(e.stats[8]),
        })
        print(f"[metrics] mAP@[.50:.95]={metrics['mAP']:.4f} | AP50={metrics['AP50']:.4f} | AR@100={metrics['AR@100']:.4f}")

    if args.metrics:
        with open(args.metrics, "w") as f:
            json.dump(metrics, f, indent=2)
        print(f"[saved] {args.metrics}")

if __name__ == "__main__":
    main()

---------------------------------------
source ~/llavaenv/bin/activate
pip install -U pycocotools

python /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/llava-1.5-7b-hf \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 500 \
  --tokens 160 \
  --out /home/vipuser/coco/llava_dt_base_500.json \
  --metrics /home/vipuser/coco/llava_metrics_base_500.json \
  --raw_debug_first 0 | tee /home/vipuser/coco/eval_base_500.log
