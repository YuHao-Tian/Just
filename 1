# /home/vipuser/image_caption.py
import sys, torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration

MODEL_DIR = "/home/vipuser/llava-1.5-7b-hf"

# 命令行参数：image_path [可选: 自定义提示词]
image_path = sys.argv[1] if len(sys.argv) > 1 else "test.jpg"
prompt_text = sys.argv[2] if len(sys.argv) > 2 else "用中文一句话描述这张图。"

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device == "cuda" else torch.float32  # A100 推荐 bfloat16

# 加载模型与处理器
model = LlavaForConditionalGeneration.from_pretrained(
    MODEL_DIR, torch_dtype=dtype, low_cpu_mem_usage=True
).eval().to(device)
processor = AutoProcessor.from_pretrained(MODEL_DIR)

# 读图
img = Image.open(image_path).convert("RGB")

# 构造“图+问句”的聊天模板（关键：包含一条 image）
messages = [{
    "role": "user",
    "content": [
        {"type": "image"},
        {"type": "text", "text": prompt_text}
]}]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)

# 编码并生成
inputs = processor(images=img, text=prompt, return_tensors="pt")
inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}

with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)

# 解码；部分模板前面会带 "ASSISTANT:"，这里顺便剥掉
text = processor.decode(outputs[0], skip_special_tokens=True)
print(text.split("ASSISTANT:")[-1].strip())

PROMPT:
python /home/vipuser/image_caption.py /path/to/your.jpg "用中文一句话描述这张图。"


# 1) 安装 Miniforge（只装到你的 home，不动系统）
cd ~
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh
bash Miniforge3-Linux-x86_64.sh -b -p $HOME/miniforge3

# 2) 让当前 shell 接管新的 conda
eval "$($HOME/miniforge3/bin/conda shell.bash hook)"
conda -V                  # 应该看到 23/24.x 而不是 4.0.5
which conda               # 应该指向 ~/miniforge3/bin/conda

# 3) 创建你要的环境（随便 3.10/3.11 都有）
conda create -n py310 python=3.10 -y
conda activate py310
python -V                 # Python 3.10.x



--------------------------------------
import os, json, argparse, time
import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def try_parse_json(text: str):
    """Extract outermost JSON object if possible; else return empty schema."""
    try:
        s, e = text.find("{"), text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
    except Exception:
        pass
    return {"detections": []}

def gen_once(model, proc, img, prompt, device, max_new_tokens=192):
    inputs = proc(images=img, text=prompt, return_tensors="pt")
    inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}
    with torch.inference_mode():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    return proc.decode(out[0], skip_special_tokens=True)

def build_canon_label(name2id):
    """Return a canonicalizer that maps raw labels to official COCO class names (lower-case)."""
    name_set = set(name2id.keys())
    # no-space map for tolerant matching like "trafficlight" -> "traffic light"
    no_space_map = {n.replace(" ", ""): n for n in name_set}

    syn = {
        "people":"person","man":"person","woman":"person","men":"person","women":"person",
        "boy":"person","girl":"person","kid":"person","child":"person","baby":"person",
        "motorbike":"motorcycle","aeroplane":"airplane","aircraft":"airplane",
        "trafficlight":"traffic light","traffic-light":"traffic light",
        "tvmonitor":"tv","tv monitor":"tv","television":"tv",
        "cellphone":"cell phone","mobile phone":"cell phone","smartphone":"cell phone","iphone":"cell phone",
        "sofa":"couch","pottedplant":"potted plant","potted plant":"potted plant",
        "diningtable":"dining table","hand bag":"handbag","hand-bag":"handbag",
        "wineglass":"wine glass","wine-glass":"wine glass",
        "tennis-racket":"tennis racket","baseball-bat":"baseball bat","baseball-glove":"baseball glove"
    }

    def canon(raw: str):
        s = (raw or "").lower().strip()
        s = s.replace("_", " ").replace("-", " ")
        while "  " in s:
            s = s.replace("  ", " ")
        if s in name_set:
            return s
        if s in syn:
            return syn[s]
        key = s.replace(" ", "")
        if key in no_space_map:
            return no_space_map[key]
        return None

    return canon

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model-dir", default="/home/vipuser/llava-1.5-7b-hf")
    ap.add_argument("--val-ann",  default="/home/vipuser/coco/annotations/instances_val2017.json")
    ap.add_argument("--val-img",  default="/home/vipuser/coco/images/val2017")
    ap.add_argument("--subset",   type=int, default=500, help="evaluate first N images (0 = all 5000)")
    ap.add_argument("--tokens",   type=int, default=192)
    ap.add_argument("--out",      default="/home/vipuser/coco/llava_dt_base.json")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype  = torch.bfloat16 if device == "cuda" else torch.float32

    # 1) Load base LLaVA-1.5-7B (no LoRA here)
    model = LlavaForConditionalGeneration.from_pretrained(args.model_dir, torch_dtype=dtype).to(device).eval()
    proc  = AutoProcessor.from_pretrained(args.model_dir)

    # 2) COCO classes & images
    coco = COCO(args.val_ann)
    cats = coco.loadCats(coco.getCatIds())
    class_names = [c["name"] for c in cats]  # official 80 names (singular, English)
    name2id = {c["name"].lower(): c["id"] for c in cats}
    canon_label = build_canon_label(name2id)

    imgs = coco.dataset["images"]
    ids  = [im["id"] for im in imgs]
    info = {im["id"]: (im["file_name"], im["width"], im["height"]) for im in imgs}
    if args.subset and args.subset > 0:
        ids = ids[:args.subset]
        print(f"[info] subset = {len(ids)} images")

    # 3) English instruction with strict constraints
    instr = (
        "You are an object detection assistant. "
        "Return a STRICT JSON object:\n"
        "{\"detections\":[{\"label\":\"<CLASS>\",\"box\":[x1,y1,x2,y2],\"confidence\":<0..1>}, ...]}\n"
        "Rules:\n"
        "1) Output ONLY JSON, no extra words.\n"
        "2) Box coordinates are NORMALIZED to [0,1], (x1,y1)=top-left, (x2,y2)=bottom-right.\n"
        "3) If nothing is found, return {\"detections\":[]}.\n"
        "4) The label MUST be one of the 80 COCO classes (singular, English): "
        + ", ".join(class_names) + ". "
        "5) At most 15 objects."
    )

    dt = []
    ok_json = 0
    t0 = time.time()

    # 4) Iterate images
    for k, img_id in enumerate(ids, 1):
        fn, W, H = info[img_id]
        img_path = os.path.join(args.val_img, fn)
        img = Image.open(img_path).convert("RGB")

        messages = [{"role":"user","content":[{"type":"image"},{"type":"text","text":instr}]}]
        prompt   = proc.apply_chat_template(messages, add_generation_prompt=True)

        txt  = gen_once(model, proc, img, prompt, device, max_new_tokens=args.tokens)
        if k <= 5:
            print(f"==== RAW OUTPUT (image {k}: {fn}) ====")
            print(txt)

        data = try_parse_json(txt)
        if not isinstance(data.get("detections", None), list):
            # one strict retry with shorter prompt
            retry = [{"role":"user","content":[
                {"type":"image"},
                {"type":"text","text":"Only output valid JSON with the key 'detections'. No extra words."}
            ]}]
            rprompt = proc.apply_chat_template(retry, add_generation_prompt=True)
            txt2 = gen_once(model, proc, img, rprompt, device, max_new_tokens=128)
            if k <= 5:
                print(f"==== RETRY OUTPUT (image {k}) ====")
                print(txt2)
            data = try_parse_json(txt2)

        dets = data.get("detections", [])
        if isinstance(dets, list):
            ok_json += 1

        # 5) Convert normalized boxes to COCO bbox and append detections
        for d in dets[:15]:
            if not isinstance(d, dict) or "box" not in d:
                continue
            lab = canon_label(d.get("label", ""))
            if lab is None:
                continue
            try:
                x1, y1, x2, y2 = d["box"]
            except Exception:
                continue
            x, y, w, h = x1 * W, y1 * H, (x2 - x1) * W, (y2 - y1) * H
            dt.append({
                "image_id": int(img_id),
                "category_id": name2id[lab],
                "bbox": [float(x), float(y), float(w), float(h)],
                "score": float(d.get("confidence", 0.9))
            })

        if k % 100 == 0:
            print(f"[{k}/{len(ids)}] dt={len(dt)}")

    # 6) Save detections & report JSON success rate
    with open(args.out, "w") as f:
        json.dump(dt, f)
    print(f"[saved] detections -> {args.out}")
    jsucc = ok_json / len(ids) if len(ids) else 0.0
    print(f"[info] JSON success rate: {ok_json}/{len(ids)} = {jsucc:.2%}")
    print(f"[time] processed {len(ids)} images in {time.time()-t0:.1f}s")

    # 7) COCO evaluation
    res = coco.loadRes(args.out)
    e = COCOeval(coco, res, "bbox")
    e.evaluate(); e.accumulate(); e.summarize()
    mAP   = float(e.stats[0])   # AP@[.50:.95]
    AP50  = float(e.stats[1])   # AP@0.50
    AR100 = float(e.stats[8])   # AR@100
    print(f"[metrics] mAP@[.50:.95]={mAP:.4f} | AP50={AP50:.4f} | AR@100={AR100:.4f}")

if __name__ == "__main__":
    main()
---------------------------------------

python /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/llava-1.5-7b-hf \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 500 \
  --out /home/vipuser/coco/llava_dt_base.json




