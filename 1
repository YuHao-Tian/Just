# /home/vipuser/image_caption.py
import sys, torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration

MODEL_DIR = "/home/vipuser/llava-1.5-7b-hf"

# 命令行参数：image_path [可选: 自定义提示词]
image_path = sys.argv[1] if len(sys.argv) > 1 else "test.jpg"
prompt_text = sys.argv[2] if len(sys.argv) > 2 else "用中文一句话描述这张图。"

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device == "cuda" else torch.float32  # A100 推荐 bfloat16

# 加载模型与处理器
model = LlavaForConditionalGeneration.from_pretrained(
    MODEL_DIR, torch_dtype=dtype, low_cpu_mem_usage=True
).eval().to(device)
processor = AutoProcessor.from_pretrained(MODEL_DIR)

# 读图
img = Image.open(image_path).convert("RGB")

# 构造“图+问句”的聊天模板（关键：包含一条 image）
messages = [{
    "role": "user",
    "content": [
        {"type": "image"},
        {"type": "text", "text": prompt_text}
]}]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)

# 编码并生成
inputs = processor(images=img, text=prompt, return_tensors="pt")
inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}

with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)

# 解码；部分模板前面会带 "ASSISTANT:"，这里顺便剥掉
text = processor.decode(outputs[0], skip_special_tokens=True)
print(text.split("ASSISTANT:")[-1].strip())
