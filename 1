# /home/vipuser/image_caption.py
import sys, torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration

MODEL_DIR = "/home/vipuser/llava-1.5-7b-hf"

# 命令行参数：image_path [可选: 自定义提示词]
image_path = sys.argv[1] if len(sys.argv) > 1 else "test.jpg"
prompt_text = sys.argv[2] if len(sys.argv) > 2 else "用中文一句话描述这张图。"

device = "cuda" if torch.cuda.is_available() else "cpu"
dtype = torch.bfloat16 if device == "cuda" else torch.float32  # A100 推荐 bfloat16

# 加载模型与处理器
model = LlavaForConditionalGeneration.from_pretrained(
    MODEL_DIR, torch_dtype=dtype, low_cpu_mem_usage=True
).eval().to(device)
processor = AutoProcessor.from_pretrained(MODEL_DIR)

# 读图
img = Image.open(image_path).convert("RGB")

# 构造“图+问句”的聊天模板（关键：包含一条 image）
messages = [{
    "role": "user",
    "content": [
        {"type": "image"},
        {"type": "text", "text": prompt_text}
]}]
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)

# 编码并生成
inputs = processor(images=img, text=prompt, return_tensors="pt")
inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}

with torch.inference_mode():
    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)

# 解码；部分模板前面会带 "ASSISTANT:"，这里顺便剥掉
text = processor.decode(outputs[0], skip_special_tokens=True)
print(text.split("ASSISTANT:")[-1].strip())

PROMPT:
python /home/vipuser/image_caption.py /path/to/your.jpg "用中文一句话描述这张图。"


# 1) 安装 Miniforge（只装到你的 home，不动系统）
cd ~
wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh
bash Miniforge3-Linux-x86_64.sh -b -p $HOME/miniforge3

# 2) 让当前 shell 接管新的 conda
eval "$($HOME/miniforge3/bin/conda shell.bash hook)"
conda -V                  # 应该看到 23/24.x 而不是 4.0.5
which conda               # 应该指向 ~/miniforge3/bin/conda

# 3) 创建你要的环境（随便 3.10/3.11 都有）
conda create -n py310 python=3.10 -y
conda activate py310
python -V                 # Python 3.10.x



--------------------------------------

# /home/vipuser/eval_llava_coco_en.py
import os, json, argparse, time
import torch
from PIL import Image
from transformers import AutoProcessor, LlavaForConditionalGeneration
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def try_parse_json(text: str):
    """Try to extract the outermost JSON object; return dict with 'detections' list."""
    try:
        s, e = text.find("{"), text.rfind("}")
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e+1])
    except Exception:
        pass
    return {"detections": []}

def generate_detections(model, proc, img, prompt, device, max_new_tokens=192):
    """One forward pass + decode + JSON parse; if parse fails, retry with a short 'JSON only' prompt."""
    inputs = proc(images=img, text=prompt, return_tensors="pt")
    inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}
    with torch.inference_mode():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    txt = proc.decode(out[0], skip_special_tokens=True)
    data = try_parse_json(txt)
    if not isinstance(data.get("detections", None), list):
        # one-shot retry with a shorter, stricter prompt
        retry_msg = [{"role":"user","content":[
            {"type":"image"},
            {"type":"text","text":"Only output valid JSON with the key 'detections'. No extra words."}
        ]}]
        retry_prompt = proc.apply_chat_template(retry_msg, add_generation_prompt=True)
        inputs = proc(images=img, text=retry_prompt, return_tensors="pt")
        inputs = {k: (v.to(device) if hasattr(v, "to") else v) for k, v in inputs.items()}
        with torch.inference_mode():
            out = model.generate(**inputs, max_new_tokens=128, do_sample=False)
        txt = proc.decode(out[0], skip_special_tokens=True)
        data = try_parse_json(txt)
        if not isinstance(data.get("detections", None), list):
            data = {"detections": []}
    return data

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model-dir", default="/home/vipuser/llava-1.5-7b-hf")
    ap.add_argument("--val-ann",  default="/home/vipuser/coco/annotations/instances_val2017.json")
    ap.add_argument("--val-img",  default="/home/vipuser/coco/images/val2017")
    ap.add_argument("--subset",   type=int, default=500, help="evaluate on first N images (0 = all 5000)")
    ap.add_argument("--tokens",   type=int, default=192)
    ap.add_argument("--out",      default="/home/vipuser/coco/llava_dt_base.json")
    ap.add_argument("--lora-dir", default=None, help="optional: LoRA adapter dir (for post-finetune eval)")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype  = torch.bfloat16 if device == "cuda" else torch.float32

    # Load base model
    model = LlavaForConditionalGeneration.from_pretrained(
        args.model_dir, torch_dtype=dtype
    ).to(device).eval()

    # Optional: attach LoRA (post-finetune)
    if args.lora_dir:
        try:
            from peft import PeftModel
            model = PeftModel.from_pretrained(model, args.lora_dir).to(device).eval()
            print(f"[info] loaded LoRA from {args.lora_dir}")
        except Exception as e:
            print("[warn] cannot load LoRA:", e)

    proc = AutoProcessor.from_pretrained(args.model_dir)

    # COCO classes and images
    coco = COCO(args.val_ann)
    cats = coco.loadCats(coco.getCatIds())
    class_names = [c["name"] for c in cats]               # 80 official English class names
    name2id = {c["name"].lower(): c["id"] for c in cats}
    name_set_lower = set(name2id.keys())

    imginfo = {im["id"]: (im["file_name"], im["width"], im["height"]) for im in coco.dataset["images"]}
    ids = list(imginfo.keys())
    if args.subset and args.subset > 0:
        ids = ids[:args.subset]
        print(f"[info] subset = {len(ids)} images")

    # English instruction with strict constraints
    INSTR = (
        "You are an object detection assistant. "
        "Return a STRICT JSON object:\n"
        "{\"detections\":[{\"label\":\"<CLASS>\",\"box\":[x1,y1,x2,y2],\"confidence\":<0..1>}, ...]}\n"
        "Requirements:\n"
        "1) Output ONLY JSON, no extra words.\n"
        "2) The box coordinates are NORMALIZED to [0,1], (x1,y1)=top-left, (x2,y2)=bottom-right.\n"
        "3) If nothing is found, return {\"detections\":[]}.\n"
        "4) The label MUST be one of the 80 COCO classes (singular, English): "
        + ", ".join(class_names) + ". "
        "5) At most 15 objects."
    )

    dt = []
    ok_json = 0
    t0 = time.time()
    for k, img_id in enumerate(ids, 1):
        fn, W, H = imginfo[img_id]
        img = Image.open(os.path.join(args.val_img, fn)).convert("RGB")

        # build chat prompt
        messages = [{"role":"user","content":[{"type":"image"},{"type":"text","text":INSTR}]}]
        prompt = proc.apply_chat_template(messages, add_generation_prompt=True)

        data = generate_detections(model, proc, img, prompt, device, max_new_tokens=args.tokens)
        dets = data.get("detections", [])
        if isinstance(dets, list):
            ok_json += 1

        # convert to COCO detection format
        for d in dets[:15]:
            if not isinstance(d, dict) or "box" not in d: continue
            lab = str(d.get("label","")).strip().lower()
            if lab not in name_set_lower:  # only accept official names
                continue
            x1, y1, x2, y2 = d["box"]
            x, y, w, h = x1*W, y1*H, (x2-x1)*W, (y2-y1)*H
            dt.append({
                "image_id": int(img_id),
                "category_id": name2id[lab],
                "bbox": [float(x), float(y), float(w), float(h)],
                "score": float(d.get("confidence", 0.9))
            })

        if k % 100 == 0:
            print(f"[{k}/{len(ids)}] dt={len(dt)}")

    with open(args.out, "w") as f:
        json.dump(dt, f)
    print(f"[info] detections saved to {args.out}")
    print(f"[info] JSON success rate: {ok_json}/{len(ids)} = {ok_json/len(ids):.2%}")
    print(f"[time] total {len(ids)} images in {time.time()-t0:.1f}s")

    # Evaluate with COCO API
    res = coco.loadRes(args.out)
    e = COCOeval(coco, res, "bbox")
    e.evaluate(); e.accumulate(); e.summarize()
    # mAP = e.stats[0]; AP50 = e.stats[1]

if __name__ == "__main__":
    main()

-----------------------------

python /home/vipuser/eval_llava_coco_en.py \
  --model-dir /home/vipuser/llava-1.5-7b-hf \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 500 \
  --out /home/vipuser/coco/llava_dt_base.json
