loading annotations into memory...
Done (t=6.54s)
creating index...
index created!
[info] subset = 10 images
==== RAW REPLY (image 1: 000000397133.jpg) ====
{"detections":[{"label":"person","box":[0.68,0.13,0.88,0.49],"confidence":0.99},{"label":"person","box":[0.68,0.13,0.88,0.49],"confidence":0.99}]}
[1/10] dt=2
==== RAW REPLY (image 2: 000000037777.jpg) ====
{"detections":[]}
[2/10] dt=2
==== RAW REPLY (image 3: 000000252219.jpg) ====
{"detections":[{"label":"person","box":[0.52, 0.3, 0.69, 0.82], "confidence":0.55}, {"label":"traffic light","box":[0.62, 0.0, 0.82, 0.22], "confidence":0.55}, {"label":"bicycle","box":[0.0, 0.49, 0.42, 0.89], "confidence":0.55}]}
[3/10] dt=5
==== RAW REPLY (image 4: 000000087038.jpg) ====
{"detections":[{"label":"person","box":[0.34,0.39,0.48,0.56],"confidence":0.85},{"label":"bicycle","box":[0.34,0.58,0.48,0.68],"confidence":0.85},{"label":"car","box":[0.0,0.58,0.1,0.68],"confidence":0.85},{"label":"motorcycle","box":[0.34,0.58,0.48,0.68],"confidence":0.85},{"label":"airplane","box":[0.0,0.58,0.1,0.68],"confidence":0.85},
[4/10] dt=5
==== RAW REPLY (image 5: 000000174482.jpg) ====
{"detections":[{"label":"bicycle","box":[0.22,0.13,0.79,0.99],"confidence":0.99},{"label":"car","box":[0.0,0.18,0.22,0.28],"confidence":0.99},{"label":"motorcycle","box":[0.54,0.19,0.75,0.34],"confidence":0.99},{"label":"truck","box":[0.79,0.18,0.92,0.28],"confidence":0.99}]}
[5/10] dt=9
[6/10] dt=9
[7/10] dt=10
[8/10] dt=11
[9/10] dt=11
[10/10] dt=11
[saved] detections -> /home/vipuser/coco/llava_dt_base.json
[info] JSON success rate: 10/10 = 100.00%
[time] processed 10 images in 22.6s
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.89s).
Accumulating evaluation results...
DONE (t=0.77s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
[metrics] mAP@[.50:.95]=0.0001 | AP50=0.0002 | AR@100=0.0000

------------------------------------------
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003
[metrics] mAP@[.50:.95]=0.0010 | AP50=0.0036 | AR@100=0.0012

-------------------------------------------------------------------------
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.011
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.012
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.034
[metrics] mAP@[.50:.95]=0.0055 | AP50=0.0180 | AR@100=0.0122

------------------------------------------------------------

Got unknown args, potentially deprecated arguments: ['--image_folder', '/home/vipuser/coco/images']
Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 55, in _training_function
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 208, in get_train_args
    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 186, in _parse_train_args
    return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 87, in _parse_args
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {unknown_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['--image_folder', '/home/vipuser/coco/images']
___________________
> import torch, sys
print("python:", sys.executable)
print("torch:", torch.__version__, "cuda_runtime_in_wheel:", torch.version.cuda) 
print("cuda_available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("device0:", torch.cuda.get_device_name(0))
    import torch.backends.cudnn as cudnn
    print("cudnn:", cudnn.version())
    # 做一次小算子，确保可用
    x = torch.randn(1024, 1024, device="cuda")
    y = x @ x.t()
    print("matmul_ok:", float(y[0,0]).__class__ is float or True)
> PY
python: /home/vipuser/miniforge3/envs/lf/bin/python
torch: 2.8.0+cu128 cuda_runtime_in_wheel: 12.8
cuda_available: True
device0: NVIDIA A100-SXM4-40GB
cudnn: 91002
matmul_ok: True
_________________________________________
[INFO|trainer.py:2433] 2025-08-19 10:04:29,322 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-19 10:04:29,322 >>   Num examples = 10
[INFO|trainer.py:2435] 2025-08-19 10:04:29,323 >>   Num Epochs = 5
[INFO|trainer.py:2436] 2025-08-19 10:04:29,323 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2439] 2025-08-19 10:04:29,323 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2440] 2025-08-19 10:04:29,323 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-19 10:04:29,323 >>   Total optimization steps = 5
[INFO|trainer.py:2442] 2025-08-19 10:04:29,332 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/5 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-08-19 10:04:31,804 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
 40%|████      | 2/5 [00:21<00:31, 10.36s/it]

[INFO|tokenization_utils_base.py:2393] 2025-08-19 10:05:22,391 >> chat template saved in trainer_output/chat_template.jinja
[INFO|tokenization_utils_base.py:2562] 2025-08-19 10:05:22,392 >> tokenizer config file saved in trainer_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2571] 2025-08-19 10:05:22,392 >> Special tokens file saved in trainer_output/special_tokens_map.json
[INFO|modelcard.py:456] 2025-08-19 10:05:22,421 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
{'train_runtime': 52.5897, 'train_samples_per_second': 1.521, 'train_steps_per_second': 0.095, 'train_loss': 1.0642495155334473, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               =  3643301GF
  train_loss               =     1.0642
  train_runtime            = 0:00:52.58
  train_samples_per_second =      1.521
  train_steps_per_second   =      0.095
_____________________________________
labels:
{"detections": [{"label": "bowl", "box": [0.24562499999999998, 0.26733021077283375, 0.27353125, 0.30437939110070256], "confidence": 1.0}, {"label": "broccoli", "box": [0.135015625, 0.6884543325526933, 0.172453125, 0.7146370023419204], "confidence": 1.0}, {"label": "bottle", "box": [0.34003125, 0.5633255269320843, 0.400953125, 0.6985714285714285], "confidence": 1.0}, {"label": "bowl", "box": [0.048875, 0.8056206088992974, 0.15531250000000002, 0.9012412177985948], "confidence": 1.0}, {"label": "dining table", "box": [0.0015625, 0.5626229508196722, 0.543171875, 1.0], "confidence": 1.0}, {"label": "cup", "box": [0.18656250000000002, 0.6381967213114754, 0.22534375, 0.718407494145199], "confidence": 1.0}, {"label": "person", "box": [0.60728125, 0.1637470725995316, 0.778234375, 0.8139110070257611], "confidence": 1.0}, {"label": "broccoli", "box": [0.154296875, 0.7137704918032787, 0.171140625, 0.726814988290398], "confidence": 1.0}, {"label": "oven", "box": [0.002125, 0.38484777517564406, 0.30300000000000005, 0.6152224824355973], "confidence": 1.0}, {"label": "knife", "box": [0.21182812499999998, 0.5841451990632318, 0.24670312499999997, 0.651569086651054], "confidence": 1.0}, {"label": "sink", "box": [0.776953125, 0.4763466042154567, 0.96759375, 0.5433489461358314], "confidence": 1.0}, {"label": "bowl", "box": [0.093171875, 0.6729742388758783, 0.21203124999999998, 0.7696955503512881], "confidence": 1.0}, {"label": "bowl", "box": [0.243703125, 0.39566744730679154, 0.284375, 0.4357845433255269], "confidence": 1.0}, {"label": "oven", "box": [0.0, 0.49391100702576113, 0.29900000000000004, 0.7257142857142858], "confidence": 1.0}, {"label": "person", "box": [0.0, 0.615480093676815, 0.09712499999999999, 0.7015925058548009], "confidence": 1.0}]}</s>
[INFO|2025-08-19 10:10:52] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.

[INFO|modeling_utils.py:1305] 2025-08-19 10:10:52,166 >> loading weights file /home/vipuser/llava-1.5-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:2411] 2025-08-19 10:10:52,167 >> Instantiating LlavaForConditionalGeneration model under default dtype torch.float16.
[INFO|configuration_utils.py:1098] 2025-08-19 10:10:52,171 >> Generate config GenerationConfig {
  "pad_token_id": 32001,
  "use_cache": false
}

[INFO|modeling_utils.py:2411] 2025-08-19 10:10:52,182 >> Instantiating CLIPVisionModel model under default dtype torch.float16.
[INFO|modeling_utils.py:2411] 2025-08-19 10:10:52,206 >> Instantiating LlamaModel model under default dtype torch.float16.
Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 12.27s/it]
[INFO|modeling_utils.py:5606] 2025-08-19 10:11:29,096 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.

[INFO|modeling_utils.py:5614] 2025-08-19 10:11:29,096 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at /home/vipuser/llava-1.5-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-08-19 10:11:29,101 >> loading configuration file /home/vipuser/llava-1.5-7b-hf/generation_config.json
[INFO|configuration_utils.py:1098] 2025-08-19 10:11:29,102 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 32001
}


[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-19 10:11:29] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-08-19 10:11:29] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,down_proj,k_proj,o_proj,q_proj,gate_proj,v_proj
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['vision_tower'].
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: multi_modal_projector.
[INFO|2025-08-19 10:11:29] llamafactory.model.loader:143 >> trainable params: 19,988,480 || all params: 7,083,415,552 || trainable%: 0.2822
[WARNING|2025-08-19 10:11:29] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[INFO|trainer.py:2433] 2025-08-19 10:11:30,078 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-19 10:11:30,078 >>   Num examples = 118,287
[INFO|trainer.py:2435] 2025-08-19 10:11:30,078 >>   Num Epochs = 3
[INFO|trainer.py:2436] 2025-08-19 10:11:30,078 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2439] 2025-08-19 10:11:30,078 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2440] 2025-08-19 10:11:30,078 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-19 10:11:30,078 >>   Total optimization steps = 22,179
[INFO|trainer.py:2442] 2025-08-19 10:11:30,082 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/22179 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-08-19 10:11:31,452 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  0%|          | 20/22179 [04:08<75:41:47, 12.30s/it]       

_______________________
python - <<'PY'
import json, re, sys, pathlib
base = "/home/vipuser/lf_data/mini500"
for name in ["train500.jsonl","val100.jsonl"]:
    src = f"{base}/{name}"
    dst = f"{base}/{name.replace('.jsonl','_noimgtok.jsonl')}"
    with open(src) as f, open(dst,"w") as g:
        for line in f:
            ex = json.loads(line)
            for m in ex["conversations"]:
                if m.get("from") == "user":
                    m["value"] = re.sub(r'^\s*<image>\s*\n?', '', m["value"])
            g.write(json.dumps(ex, ensure_ascii=False) + "\n")
print("done")
PY

# 写一个对应的 dataset_info.json（指向 *_noimgtok.jsonl）
cat > /home/vipuser/lf_data/mini500/dataset_info.json <<'JSON'
{
  "coco_detjson_train500": {
    "file_name": "train500_noimgtok.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  },
  "coco_detjson_val100": {
    "file_name": "val100_noimgtok.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  }
}
JSON

export PYTORCH_SDP_KERNEL=mem_efficient
export NVIDIA_TF32_OVERRIDE=1
export OMP_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false
export HF_DATASETS_CACHE=/home/vipuser/hf_cache

OUT=/home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs

~/miniforge3/envs/lf/bin/llamafactory-cli train \
  --stage sft --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data/mini500 \
  --dataset coco_detjson_train500 \
  --template llava \
  --cutoff_len 2048 \                         # 提回 2048，避免截断图像 token
  --bf16 true --tf32 true --optim adamw_torch_fused \
  --per_device_train_batch_size 2 --gradient_accumulation_steps 8 \
  --num_train_epochs 1 --learning_rate 5e-05 \
  --lora_rank 8 --lora_alpha 16 --lora_dropout 0.1 --lora_target all \
  --do_train true --do_eval false \
  --dataloader_num_workers 8 --preprocessing_num_workers 8 \
  --output_dir $OUT --overwrite_output_dir true \
  2>&1 | tee /home/vipuser/saves/train_500imgs.log

