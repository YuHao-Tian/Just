loading annotations into memory...
Done (t=6.54s)
creating index...
index created!
[info] subset = 10 images
==== RAW REPLY (image 1: 000000397133.jpg) ====
{"detections":[{"label":"person","box":[0.68,0.13,0.88,0.49],"confidence":0.99},{"label":"person","box":[0.68,0.13,0.88,0.49],"confidence":0.99}]}
[1/10] dt=2
==== RAW REPLY (image 2: 000000037777.jpg) ====
{"detections":[]}
[2/10] dt=2
==== RAW REPLY (image 3: 000000252219.jpg) ====
{"detections":[{"label":"person","box":[0.52, 0.3, 0.69, 0.82], "confidence":0.55}, {"label":"traffic light","box":[0.62, 0.0, 0.82, 0.22], "confidence":0.55}, {"label":"bicycle","box":[0.0, 0.49, 0.42, 0.89], "confidence":0.55}]}
[3/10] dt=5
==== RAW REPLY (image 4: 000000087038.jpg) ====
{"detections":[{"label":"person","box":[0.34,0.39,0.48,0.56],"confidence":0.85},{"label":"bicycle","box":[0.34,0.58,0.48,0.68],"confidence":0.85},{"label":"car","box":[0.0,0.58,0.1,0.68],"confidence":0.85},{"label":"motorcycle","box":[0.34,0.58,0.48,0.68],"confidence":0.85},{"label":"airplane","box":[0.0,0.58,0.1,0.68],"confidence":0.85},
[4/10] dt=5
==== RAW REPLY (image 5: 000000174482.jpg) ====
{"detections":[{"label":"bicycle","box":[0.22,0.13,0.79,0.99],"confidence":0.99},{"label":"car","box":[0.0,0.18,0.22,0.28],"confidence":0.99},{"label":"motorcycle","box":[0.54,0.19,0.75,0.34],"confidence":0.99},{"label":"truck","box":[0.79,0.18,0.92,0.28],"confidence":0.99}]}
[5/10] dt=9
[6/10] dt=9
[7/10] dt=10
[8/10] dt=11
[9/10] dt=11
[10/10] dt=11
[saved] detections -> /home/vipuser/coco/llava_dt_base.json
[info] JSON success rate: 10/10 = 100.00%
[time] processed 10 images in 22.6s
Loading and preparing results...
DONE (t=0.00s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.89s).
Accumulating evaluation results...
DONE (t=0.77s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000
[metrics] mAP@[.50:.95]=0.0001 | AP50=0.0002 | AR@100=0.0000

------------------------------------------
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003
[metrics] mAP@[.50:.95]=0.0010 | AP50=0.0036 | AR@100=0.0012

-------------------------------------------------------------------------
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.005
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.018
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.011
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.012
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.012
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.034
[metrics] mAP@[.50:.95]=0.0055 | AP50=0.0180 | AR@100=0.0122

------------------------------------------------------------

Got unknown args, potentially deprecated arguments: ['--image_folder', '/home/vipuser/coco/images']
Traceback (most recent call last):
  File "/home/vipuser/llavaenv/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/train/tuner.py", line 55, in _training_function
    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 208, in get_train_args
    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 186, in _parse_train_args
    return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
  File "/home/vipuser/llavaenv/lib/python3.10/site-packages/llamafactory/hparams/parser.py", line 87, in _parse_args
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {unknown_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['--image_folder', '/home/vipuser/coco/images']
___________________
> import torch, sys
print("python:", sys.executable)
print("torch:", torch.__version__, "cuda_runtime_in_wheel:", torch.version.cuda) 
print("cuda_available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("device0:", torch.cuda.get_device_name(0))
    import torch.backends.cudnn as cudnn
    print("cudnn:", cudnn.version())
    # 做一次小算子，确保可用
    x = torch.randn(1024, 1024, device="cuda")
    y = x @ x.t()
    print("matmul_ok:", float(y[0,0]).__class__ is float or True)
> PY
python: /home/vipuser/miniforge3/envs/lf/bin/python
torch: 2.8.0+cu128 cuda_runtime_in_wheel: 12.8
cuda_available: True
device0: NVIDIA A100-SXM4-40GB
cudnn: 91002
matmul_ok: True
_________________________________________
[INFO|trainer.py:2433] 2025-08-19 10:04:29,322 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-19 10:04:29,322 >>   Num examples = 10
[INFO|trainer.py:2435] 2025-08-19 10:04:29,323 >>   Num Epochs = 5
[INFO|trainer.py:2436] 2025-08-19 10:04:29,323 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2439] 2025-08-19 10:04:29,323 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2440] 2025-08-19 10:04:29,323 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-19 10:04:29,323 >>   Total optimization steps = 5
[INFO|trainer.py:2442] 2025-08-19 10:04:29,332 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/5 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-08-19 10:04:31,804 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
 40%|████      | 2/5 [00:21<00:31, 10.36s/it]

[INFO|tokenization_utils_base.py:2393] 2025-08-19 10:05:22,391 >> chat template saved in trainer_output/chat_template.jinja
[INFO|tokenization_utils_base.py:2562] 2025-08-19 10:05:22,392 >> tokenizer config file saved in trainer_output/tokenizer_config.json
[INFO|tokenization_utils_base.py:2571] 2025-08-19 10:05:22,392 >> Special tokens file saved in trainer_output/special_tokens_map.json
[INFO|modelcard.py:456] 2025-08-19 10:05:22,421 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
{'train_runtime': 52.5897, 'train_samples_per_second': 1.521, 'train_steps_per_second': 0.095, 'train_loss': 1.0642495155334473, 'epoch': 5.0}
***** train metrics *****
  epoch                    =        5.0
  total_flos               =  3643301GF
  train_loss               =     1.0642
  train_runtime            = 0:00:52.58
  train_samples_per_second =      1.521
  train_steps_per_second   =      0.095
_____________________________________
labels:
{"detections": [{"label": "bowl", "box": [0.24562499999999998, 0.26733021077283375, 0.27353125, 0.30437939110070256], "confidence": 1.0}, {"label": "broccoli", "box": [0.135015625, 0.6884543325526933, 0.172453125, 0.7146370023419204], "confidence": 1.0}, {"label": "bottle", "box": [0.34003125, 0.5633255269320843, 0.400953125, 0.6985714285714285], "confidence": 1.0}, {"label": "bowl", "box": [0.048875, 0.8056206088992974, 0.15531250000000002, 0.9012412177985948], "confidence": 1.0}, {"label": "dining table", "box": [0.0015625, 0.5626229508196722, 0.543171875, 1.0], "confidence": 1.0}, {"label": "cup", "box": [0.18656250000000002, 0.6381967213114754, 0.22534375, 0.718407494145199], "confidence": 1.0}, {"label": "person", "box": [0.60728125, 0.1637470725995316, 0.778234375, 0.8139110070257611], "confidence": 1.0}, {"label": "broccoli", "box": [0.154296875, 0.7137704918032787, 0.171140625, 0.726814988290398], "confidence": 1.0}, {"label": "oven", "box": [0.002125, 0.38484777517564406, 0.30300000000000005, 0.6152224824355973], "confidence": 1.0}, {"label": "knife", "box": [0.21182812499999998, 0.5841451990632318, 0.24670312499999997, 0.651569086651054], "confidence": 1.0}, {"label": "sink", "box": [0.776953125, 0.4763466042154567, 0.96759375, 0.5433489461358314], "confidence": 1.0}, {"label": "bowl", "box": [0.093171875, 0.6729742388758783, 0.21203124999999998, 0.7696955503512881], "confidence": 1.0}, {"label": "bowl", "box": [0.243703125, 0.39566744730679154, 0.284375, 0.4357845433255269], "confidence": 1.0}, {"label": "oven", "box": [0.0, 0.49391100702576113, 0.29900000000000004, 0.7257142857142858], "confidence": 1.0}, {"label": "person", "box": [0.0, 0.615480093676815, 0.09712499999999999, 0.7015925058548009], "confidence": 1.0}]}</s>
[INFO|2025-08-19 10:10:52] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.

[INFO|modeling_utils.py:1305] 2025-08-19 10:10:52,166 >> loading weights file /home/vipuser/llava-1.5-7b-hf/model.safetensors.index.json
[INFO|modeling_utils.py:2411] 2025-08-19 10:10:52,167 >> Instantiating LlavaForConditionalGeneration model under default dtype torch.float16.
[INFO|configuration_utils.py:1098] 2025-08-19 10:10:52,171 >> Generate config GenerationConfig {
  "pad_token_id": 32001,
  "use_cache": false
}

[INFO|modeling_utils.py:2411] 2025-08-19 10:10:52,182 >> Instantiating CLIPVisionModel model under default dtype torch.float16.
[INFO|modeling_utils.py:2411] 2025-08-19 10:10:52,206 >> Instantiating LlamaModel model under default dtype torch.float16.
Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 12.27s/it]
[INFO|modeling_utils.py:5606] 2025-08-19 10:11:29,096 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.

[INFO|modeling_utils.py:5614] 2025-08-19 10:11:29,096 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at /home/vipuser/llava-1.5-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:1051] 2025-08-19 10:11:29,101 >> loading configuration file /home/vipuser/llava-1.5-7b-hf/generation_config.json
[INFO|configuration_utils.py:1098] 2025-08-19 10:11:29,102 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 32001
}


[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-08-19 10:11:29] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-08-19 10:11:29] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,down_proj,k_proj,o_proj,q_proj,gate_proj,v_proj
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['vision_tower'].
[INFO|2025-08-19 10:11:29] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: multi_modal_projector.
[INFO|2025-08-19 10:11:29] llamafactory.model.loader:143 >> trainable params: 19,988,480 || all params: 7,083,415,552 || trainable%: 0.2822
[WARNING|2025-08-19 10:11:29] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[INFO|trainer.py:2433] 2025-08-19 10:11:30,078 >> ***** Running training *****
[INFO|trainer.py:2434] 2025-08-19 10:11:30,078 >>   Num examples = 118,287
[INFO|trainer.py:2435] 2025-08-19 10:11:30,078 >>   Num Epochs = 3
[INFO|trainer.py:2436] 2025-08-19 10:11:30,078 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2439] 2025-08-19 10:11:30,078 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2440] 2025-08-19 10:11:30,078 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2441] 2025-08-19 10:11:30,078 >>   Total optimization steps = 22,179
[INFO|trainer.py:2442] 2025-08-19 10:11:30,082 >>   Number of trainable parameters = 19,988,480
  0%|          | 0/22179 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-08-19 10:11:31,452 >> `loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
  0%|          | 20/22179 [04:08<75:41:47, 12.30s/it]       

_______________________
python - <<'PY'
import json, re, sys, pathlib
base = "/home/vipuser/lf_data/mini500"
for name in ["train500.jsonl","val100.jsonl"]:
    src = f"{base}/{name}"
    dst = f"{base}/{name.replace('.jsonl','_noimgtok.jsonl')}"
    with open(src) as f, open(dst,"w") as g:
        for line in f:
            ex = json.loads(line)
            for m in ex["conversations"]:
                if m.get("from") == "user":
                    m["value"] = re.sub(r'^\s*<image>\s*\n?', '', m["value"])
            g.write(json.dumps(ex, ensure_ascii=False) + "\n")
print("done")
PY

# 写一个对应的 dataset_info.json（指向 *_noimgtok.jsonl）
cat > /home/vipuser/lf_data/mini500/dataset_info.json <<'JSON'
{
  "coco_detjson_train500": {
    "file_name": "train500_noimgtok.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  },
  "coco_detjson_val100": {
    "file_name": "val100_noimgtok.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  }
}
JSON

export PYTORCH_SDP_KERNEL=mem_efficient
export NVIDIA_TF32_OVERRIDE=1
export OMP_NUM_THREADS=1
export TOKENIZERS_PARALLELISM=false
export HF_DATASETS_CACHE=/home/vipuser/hf_cache

OUT=/home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs

~/miniforge3/envs/lf/bin/llamafactory-cli train \
  --stage sft --finetuning_type lora \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --dataset_dir /home/vipuser/lf_data/mini500 \
  --dataset coco_detjson_train500 \
  --template llava \
  --cutoff_len 2048 \                         # 提回 2048，避免截断图像 token
  --bf16 true --tf32 true --optim adamw_torch_fused \
  --per_device_train_batch_size 2 --gradient_accumulation_steps 8 \
  --num_train_epochs 1 --learning_rate 5e-05 \
  --lora_rank 8 --lora_alpha 16 --lora_dropout 0.1 --lora_target all \
  --do_train true --do_eval false \
  --dataloader_num_workers 8 --preprocessing_num_workers 8 \
  --output_dir $OUT --overwrite_output_dir true \
  2>&1 | tee /home/vipuser/saves/train_500imgs.log

_________________

PYTORCH_SDP_KERNEL=mem_efficient NVIDIA_TF32_OVERRIDE=1 HF_DATASETS_CACHE=/home/vipuser/hf_cache OMP_NUM_THREADS=1 TOKENIZERS_PARALLELISM=false \
~/miniforge3/envs/lf/bin/llamafactory-cli train --stage sft --finetuning_type lora \
 --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
 --dataset_dir /home/vipuser/lf_data/mini500 --dataset coco_detjson_train500 \
 --template llava --cutoff_len 2048 --bf16 true --tf32 true --optim adamw_torch_fused \
 --per_device_train_batch_size 2 --gradient_accumulation_steps 8 --num_train_epochs 1 --learning_rate 5e-05 \
 --lora_rank 8 --lora_alpha 16 --lora_dropout 0.1 --lora_target all --do_train true --do_eval false \
 --dataloader_num_workers 8 --preprocessing_num_workers 8 \
 --output_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs --overwrite_output_dir true \
 2>&1 | tee /home/vipuser/saves/train_500imgs.log
________________
ValueError: The number of images does not match the number of <image> tokens in [{'content': 'You are an object detection assistant. Return ONLY a valid JSON object with key \'detections\'. Each item: {"label": <one of COCO-80>, "box": [x1,y1,x2,y2], "confidence": [0,1]}. Coordinates are normalized to [0,1] with (x1,y1)=top-left and (x2,y2)=bottom-right. If nothing is found, return {"detections":[]}. Output JSON only.', 'role': 'user'}, {'content': '{"detections": [{"label": "giraffe", "box": [0.30384976525821594, 0.21798437499999998, 1.0, 0.9056249999999999], "confidence": 1.0}, {"label": "giraffe", "box": [0.06068075117370892, 0.1414375, 0.9840140845070422, 0.88734375], "confidence": 1.0}]}', 'role': 'assistant'}].
——————————————————————————————————————————
1.
python - <<'PY'
import json, re, pathlib

base = pathlib.Path("/home/vipuser/lf_data/mini500")
in_train = base/"train500.jsonl"
in_val   = base/"val100.jsonl"     # 若没有 val100.jsonl，可以忽略这一行和下面相应处理
out_train = base/"train500_norm.jsonl"
out_val   = base/"val100_norm.jsonl"

def norm_line(js):
    # 1) 保证 images 恰好 1 张
    imgs = js.get("images") or js.get("image") or []
    if isinstance(imgs, str): imgs = [imgs]
    assert isinstance(imgs, list), "images 字段必须是列表"
    assert len(imgs) >= 1, "每条样本必须至少 1 张图"
    js["images"] = [imgs[0]]  # 只保留第一张

    # 2) 只改“用户”那条消息：确保恰好 1 个 <image>，放在开头
    conv = js["conversations"]
    # 找第一条用户消息
    for m in conv:
        if m.get("from") == "user":
            # 去掉所有已有的 <image>（可能有多个或位置不对）
            txt = re.sub(r'\s*<image>\s*\n?', '', m["value"])
            # 重新在最前面补上 1 个
            m["value"] = "<image>\n" + txt.strip()
            break
    return js

def process(fin, fout):
    if not fin.exists(): return
    with fin.open() as f, fout.open("w") as g:
        for line in f:
            js = json.loads(line)
            js = norm_line(js)
            g.write(json.dumps(js, ensure_ascii=False) + "\n")

process(in_train, out_train)
process(in_val, out_val)
print("done")
PY

2.
{
  "coco_detjson_train500": {
    "file_name": "train500_norm.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  },
  "coco_detjson_val100": {
    "file_name": "val100_norm.jsonl",
    "formatting": "sharegpt",
    "columns": { "messages": "conversations", "images": "images" },
    "tags": { "role_tag": "from", "content_tag": "value", "user_tag": "user", "assistant_tag": "assistant" },
    "multi_modal": true
  }
}

3.
python - <<'PY'
import json, re, sys, pathlib
base="/home/vipuser/lf_data/mini500"
bad=0
for name in ["train500_norm.jsonl","val100_norm.jsonl"]:
    p=pathlib.Path(base)/name
    if not p.exists(): continue
    with p.open() as f:
        for ln,line in enumerate(f,1):
            js=json.loads(line)
            n_img = len(js.get("images",[]))
            txt=" ".join(m["value"] for m in js["conversations"] if m.get("from")=="user")
            n_tok = len(re.findall(r"<image>", txt))
            if n_img!=1 or n_tok!=1:
                bad+=1
                if bad<=5: print("BAD:", name, "line", ln, "n_img", n_img, "n_tok", n_tok)
print("done; bad=", bad)
PY
4.
PYTORCH_SDP_KERNEL=math NVIDIA_TF32_OVERRIDE=1 HF_DATASETS_CACHE=/home/vipuser/hf_cache OMP_NUM_THREADS=1 TOKENIZERS_PARALLELISM=false \
~/miniforge3/envs/lf/bin/llamafactory-cli train --stage sft --finetuning_type lora \
 --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
 --dataset_dir /home/vipuser/lf_data/mini500 --dataset coco_detjson_train500 \
 --template llava --cutoff_len 2048 --bf16 true --tf32 true \
 --per_device_train_batch_size 2 --gradient_accumulation_steps 8 --num_train_epochs 1 --learning_rate 5e-05 \
 --lora_rank 8 --lora_alpha 16 --lora_dropout 0.1 --lora_target all --do_train true --do_eval false \
 --dataloader_num_workers 8 --preprocessing_num_workers 8 --overwrite_cache true \
 --output_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs --overwrite_output_dir true \
 2>&1 | tee /home/vipuser/saves/train_500imgs.log
_____________________________________________________________________
# 合并
~/miniforge3/envs/lf/bin/llamafactory-cli merge-lora \
  --model_name_or_path   /home/vipuser/llava-1.5-7b-hf \
  --adapter_name_or_path /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs \
  --output_dir           /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged

# 先跑 500 张 val 子集看趋势
PYTORCH_SDP_KERNEL=math \
python -u /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 500 --tokens 160 \
  --out /home/vipuser/coco/llava_dt_lora_500imgs_subset.json

# 看起来靠谱，再跑全量 val2017
PYTORCH_SDP_KERNEL=math \
python -u /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --tokens 160 \
  --out /home/vipuser/coco/llava_dt_lora_500imgs_full.json

_______________
(lf) vipuser@ubuntu22:~$ ~/miniforge3/envs/lf/bin/llamafactory-cli merge-lora \
  --model_name_or_path   /home/vipuser/llava-1.5-7b-hf \
  --adapter_name_or_path /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs \
  --output_dir           /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged
/home/vipuser/miniforge3/envs/lf/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
Unknown command: merge-lora.
----------------------------------------------------------------------
| Usage:                                                             |
|   llamafactory-cli api -h: launch an OpenAI-style API server       |
|   llamafactory-cli chat -h: launch a chat interface in CLI         |
|   llamafactory-cli eval -h: evaluate models                        |
|   llamafactory-cli export -h: merge LoRA adapters and export model |
|   llamafactory-cli train -h: train models                          |
|   llamafactory-cli webchat -h: launch a chat interface in Web UI   |
|   llamafactory-cli webui: launch LlamaBoard                        |
|   llamafactory-cli version: show version info                      |
----------------------------------------------------------------------
(lf) vipuser@ubuntu22:~$ 

~/miniforge3/envs/lf/bin/llamafactory-cli export \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --adapter_name_or_path /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs \
  --export_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged

ls -lah /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged

[WARNING|2025-08-19 11:54:02] llamafactory.data.template:148 >> `template` was not specified, try parsing the chat template from the tokenizer.
Traceback (most recent call last):
  File "/home/vipuser/miniforge3/envs/lf/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
  File "/home/vipuser/LLaMA-Factory/src/llamafactory/cli.py", line 151, in main
    COMMAND_MAP[command]()
  File "/home/vipuser/LLaMA-Factory/src/llamafactory/train/tuner.py", line 125, in export_model
    template = get_template_and_fix_tokenizer(tokenizer, data_args)
  File "/home/vipuser/LLaMA-Factory/src/llamafactory/data/template.py", line 596, in get_template_and_fix_tokenizer
    template = parse_template(tokenizer)
  File "/home/vipuser/LLaMA-Factory/src/llamafactory/data/template.py", line 573, in parse_template
    format_user=StringFormatter(slots=[user_slot]),
  File "<string>", line 5, in __init__
  File "/home/vipuser/LLaMA-Factory/src/llamafactory/data/formatter.py", line 70, in __post_init__
    raise ValueError("A placeholder is required in the string formatter.")
ValueError: A placeholder is required in the string formatter.

~/miniforge3/envs/lf/bin/llamafactory-cli export \
  --model_name_or_path /home/vipuser/llava-1.5-7b-hf \
  --adapter_name_or_path /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs \
  --export_dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged \
  --template llava
_______________________
(lf) vipuser@ubuntu22:~$ PYTORCH_SDP_KERNEL=math \
python -u /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 500 --tokens 160 \
  --out /home/vipuser/coco/llava_dt_lora_500imgs_subset.json
Traceback (most recent call last):
  File "/home/vipuser/eval_llava_coco_pretrain.py", line 5, in <module>
    from pycocotools.coco import COCO
ModuleNotFoundError: No module named 'pycocotools'
——————————————————————————————
source ~/miniforge3/etc/profile.d/conda.sh
conda activate lf

PYTORCH_SDP_KERNEL=math \
python -u /home/vipuser/eval_llava_coco_pretrain.py \
  --model-dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 50 \
  --tokens 512 \
  --out /home/vipuser/coco/llava_dt_lora_tokens512_subset50.json

___________________

# LoRA（合并后：用 *_merged 这个目录）
python -u /home/vipuser/eval_llava_coco_chat_SOLID.py \
  --model-dir /home/vipuser/saves/lf_llava15_7b_coco_loraB_500imgs_merged \
  --val-ann  /home/vipuser/coco/annotations/instances_val2017.json \
  --val-img  /home/vipuser/coco/images/val2017 \
  --subset 10 \
  --tokens 512 --min-conf 0.00 --nms-iou 0.60 --max-objects 50 \
  --out /home/vipuser/coco/LORA_500.json --save-raw
___________________
baseline running 500 val images result:
DONE (t=1.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004
==== SUMMARY ====
mAP@[.50:.95]: 0.0021
AP50:          0.0061
AR@100:        0.0019

Lora(after trained with 500 images)running 500 val images result:
